{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.io as io\n",
    "from sklearn.model_selection import train_test_split\n",
    "from os import path as path\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, Dense, Flatten, Reshape, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import time\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from itertools import zip_longest\n",
    "import csv\n",
    "import qkeras as qkeras\n",
    "from qkeras import *\n",
    "from qkeras.utils import model_save_quantized_weights\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"events.pickle\", 'rb') as fp:\n",
    "    events = pickle.load(fp)\n",
    "\n",
    "with open(\"sorted_ch.pkl\", 'rb') as fp:\n",
    "    sorted_ch = pickle.load(fp)\n",
    "    \n",
    "channel_groups ={   \n",
    "    1: {   \n",
    "        'channels': list(range(64)),\n",
    "        'geometry': {  \n",
    "            0: [0, 400],\n",
    "            1: [0, 775],\n",
    "            2: [0, 425],\n",
    "            3: [0, 750],\n",
    "            4: [0, 725],\n",
    "            5: [0, 450],\n",
    "            6: [0, 700],\n",
    "            7: [0, 475],\n",
    "            8: [250, 275],\n",
    "            9: [250, 100],\n",
    "            10: [250, 250],\n",
    "            11: [250, 125],\n",
    "            12: [250, 225],\n",
    "            13: [250, 150],\n",
    "            14: [250, 525],\n",
    "            15: [250, 175],\n",
    "            16: [250, 0],\n",
    "            17: [250, 375],\n",
    "            18: [250, 25],\n",
    "            19: [250, 350],\n",
    "            20: [250, 325],\n",
    "            21: [250, 50],\n",
    "            22: [250, 300],\n",
    "            23: [250, 75],\n",
    "            24: [0, 675],\n",
    "            25: [0, 500],\n",
    "            26: [0, 650],\n",
    "            27: [0, 525],\n",
    "            28: [0, 625],\n",
    "            29: [0, 550],\n",
    "            30: [0, 125],\n",
    "            31: [0, 575],\n",
    "            32: [250, 650],\n",
    "            33: [250, 550],\n",
    "            34: [250, 625],\n",
    "            35: [250, 575],\n",
    "            36: [250, 775],\n",
    "            37: [250, 400],\n",
    "            38: [250, 750],\n",
    "            39: [250, 425],\n",
    "            40: [0, 325],\n",
    "            41: [0, 50],\n",
    "            42: [0, 100],\n",
    "            43: [0, 275],\n",
    "            44: [0, 75],\n",
    "            45: [0, 300],\n",
    "            46: [0, 600],\n",
    "            47: [0, 200],\n",
    "            48: [0, 250],\n",
    "            49: [0, 150],\n",
    "            50: [0, 225],\n",
    "            51: [0, 175],\n",
    "            52: [0, 375],\n",
    "            53: [0, 0],\n",
    "            54: [0, 350],\n",
    "            55: [0, 25],\n",
    "            56: [250, 725],\n",
    "            57: [250, 450],\n",
    "            58: [250, 500],\n",
    "            59: [250, 675],\n",
    "            60: [250, 475],\n",
    "            61: [250, 700],\n",
    "            62: [250, 200],\n",
    "            63: [250, 600]},\n",
    "        'graph': []\n",
    "        }\n",
    "    }\n",
    "channel_cord = channel_groups[1][\"geometry\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-numbering the channel-numbers\n",
    "events_dict = {}\n",
    "ch_dict = {}\n",
    "for i, k in enumerate(events):\n",
    "    events_dict[i] = events[k]\n",
    "    ch_dict[i] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([1, 2, 3, 4, 5, 6, 7, 8, 11, 12, 13, 14, 15, 16, 23, 28, 44, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56])\n",
      "shape = (# of data, input recording channels, snippet sample size)\n",
      "0 (300, 49, 180)\n",
      "1 (300, 49, 180)\n",
      "2 (297, 49, 180)\n",
      "3 (300, 49, 180)\n",
      "4 (300, 49, 180)\n",
      "5 (300, 49, 180)\n",
      "6 (300, 49, 180)\n",
      "7 (300, 49, 180)\n",
      "8 (209, 49, 180)\n",
      "9 (300, 49, 180)\n",
      "10 (300, 49, 180)\n",
      "11 (300, 49, 180)\n",
      "12 (300, 49, 180)\n",
      "13 (300, 49, 180)\n",
      "14 (300, 49, 180)\n",
      "15 (212, 49, 180)\n",
      "16 (300, 49, 180)\n",
      "17 (300, 49, 180)\n",
      "18 (234, 49, 180)\n",
      "19 (300, 49, 180)\n",
      "20 (231, 49, 180)\n",
      "21 (300, 49, 180)\n",
      "22 (300, 49, 180)\n",
      "23 (300, 49, 180)\n",
      "24 (300, 49, 180)\n",
      "25 (300, 49, 180)\n",
      "26 (300, 49, 180)\n"
     ]
    }
   ],
   "source": [
    "print(events.keys())\n",
    "print(\"shape = (# of data, input recording channels, snippet sample size)\")\n",
    "for i, k in enumerate(events_dict):\n",
    "    print(k,events_dict[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Take middle 1 ms of the snippet ###\n",
    "shorten_dict = dict()\n",
    "for key in events_dict:\n",
    "    shorten_dict[key] = events_dict[key][:,:,75:105]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Train, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### split snippet into sub-snippets of size filter_width ###\n",
    "def convert2D(data, filter_width):\n",
    "    org_shape = data.shape\n",
    "    X_2d = np.reshape(data, (org_shape[0], org_shape[1], -1, filter_width))\n",
    "    X_2d = np.moveaxis(X_2d, [2,3],[3,2])\n",
    "    return X_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define model ###\n",
    "def create_model_2d(all_classes, filter_width, inp_shape):\n",
    "    # inputshape = (49, 6, 15)\n",
    "    model = Sequential()\n",
    "    filter_cout = 2\n",
    "    model.add(Conv2D(filters=filter_cout, kernel_size=(4,4), strides = (1,2), activation='relu', input_shape=inp_shape))\n",
    "    model.add(Reshape((-1, filter_cout)))\n",
    "    model.add(Conv1D(filters=2, kernel_size=4, strides = 2, activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(all_classes, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5835, 49, 6, 5) (5835, 1) (778, 49, 6, 5) (778, 1) (1170, 49, 6, 5) (1170, 1)\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26]\n"
     ]
    }
   ],
   "source": [
    "### unifromly distribute units into test, train, validation ###\n",
    "filter_width = 6\n",
    "\n",
    "np.random.seed(2021)\n",
    "x_data = []\n",
    "y_data = []\n",
    "x_train_reshaped = []\n",
    "x_test_reshaped = []\n",
    "x_val_reshaped = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "y_val = []\n",
    "\n",
    "train_ratio = 0.75\n",
    "validation_ratio = 0.1\n",
    "test_ratio = 0.15\n",
    "\n",
    "# events_dict_avg\n",
    "events_dict_reDetect = shorten_dict\n",
    "for unit in events_dict_reDetect:\n",
    "    unit_waveform = []\n",
    "    unit_y = []\n",
    "    for waveforms in events_dict_reDetect[unit]:\n",
    "        unit_waveform.append(waveforms)\n",
    "        unit_y.append(unit)\n",
    "        \n",
    "    unit_waveform = np.array(unit_waveform)\n",
    "    unit_y = np.array(unit_y)\n",
    "\n",
    "    # train is now 75% of the entire data set\n",
    "    x_train_N, x_test_N, y_train_N, y_test_N = train_test_split(unit_waveform, unit_y, test_size=1 - train_ratio, random_state=2021)\n",
    "\n",
    "    # test is now 10% of the initial data set\n",
    "    # validation is now 15% of the initial data set\n",
    "    x_val_N, x_test_N, y_val_N, y_test_N = train_test_split(x_test_N, y_test_N, test_size=test_ratio/(test_ratio + validation_ratio), random_state=2021)\n",
    "    \n",
    "\n",
    "    y_train.append(y_train_N)\n",
    "    y_test.append(y_test_N)\n",
    "    y_val.append(y_val_N)\n",
    "\n",
    "    x_train_reshaped.append(np.moveaxis(x_train_N, [1,2], [2,1]))\n",
    "    x_test_reshaped.append(np.moveaxis(x_test_N, [1,2], [2,1]))\n",
    "    x_val_reshaped.append(np.moveaxis(x_val_N, [1,2], [2,1]))\n",
    "\n",
    "y_train = np.concatenate(y_train, axis=0)\n",
    "y_test = np.concatenate(y_test, axis=0)\n",
    "y_val = np.concatenate(y_val, axis=0)\n",
    "y_train = y_train.reshape((y_train.shape[0],1))\n",
    "y_test = y_test.reshape((y_test.shape[0],1))\n",
    "y_val = y_val.reshape((y_val.shape[0],1))\n",
    "\n",
    "x_train_reshaped = np.concatenate(x_train_reshaped, axis=0)\n",
    "x_test_reshaped = np.concatenate(x_test_reshaped, axis=0)\n",
    "x_val_reshaped = np.concatenate(x_val_reshaped, axis=0)\n",
    "\n",
    "x_train_reshaped = np.moveaxis(x_train_reshaped, [1,2], [2,1])\n",
    "x_test_reshaped = np.moveaxis(x_test_reshaped, [1,2], [2,1])\n",
    "x_val_reshaped = np.moveaxis(x_val_reshaped, [1,2], [2,1])\n",
    "\n",
    "x_train_reshaped = x_train_reshaped[:,sorted_ch,:]\n",
    "x_test_reshaped = x_test_reshaped[:,sorted_ch,:]\n",
    "x_val_reshaped = x_val_reshaped[:,sorted_ch,:]\n",
    "\n",
    "x_train_reshaped = convert2D(x_train_reshaped, filter_width)\n",
    "x_test_reshaped = convert2D(x_test_reshaped, filter_width)\n",
    "x_val_reshaped = convert2D(x_val_reshaped, filter_width)\n",
    "\n",
    "\n",
    "print(x_train_reshaped.shape, y_train.shape, x_val_reshaped.shape, y_val.shape, x_test_reshaped.shape, y_test.shape)\n",
    "print(np.unique(y_train))\n",
    "\n",
    "all_classes = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### train and test models multiple trials ###\n",
    "perGroup_list = []\n",
    "headers = ['Partition', 'trail', 'test_acc', 'val_acc', 'val_loss', 'train_loss']\n",
    "max_test_acc = 0\n",
    "num_trials = 10\n",
    "for i in range(num_trials):\n",
    "    print(i)\n",
    "    perGroup_dict = dict()\n",
    "    perGroup_dict['Partition'] = i\n",
    "    model = create_model_2d(all_classes, filter_width, x_train_reshaped[0].shape)\n",
    "    model.summary()\n",
    "    model.compile(\n",
    "    loss=keras.losses.categorical_crossentropy,\n",
    "                        optimizer=keras.optimizers.Adam(lr=0.0005), ## (0.001) originally\n",
    "                        metrics=['accuracy'],\n",
    "    )\n",
    "\n",
    "    model_file_name = './models/CNN-s_explore_trial' + str(i) + '.wts.h5'\n",
    "\n",
    "    # Train the model.\n",
    "    t0 = time.time()\n",
    "    history = model.fit(\n",
    "        x_train_reshaped,\n",
    "        to_categorical(y_train),\n",
    "        epochs=600,\n",
    "        validation_data=(x_val_reshaped, to_categorical(y_val)),\n",
    "        verbose=1,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100),\n",
    "            keras.callbacks.ModelCheckpoint(model_file_name, monitor='val_loss', verbose=0, save_best_only=True, mode='auto')]\n",
    "        )\n",
    "    t1 = time.time()\n",
    "    duration_ms = (t1-t0)*1000\n",
    "\n",
    "    history_data = {'epoch':history.epoch, 'history':history.history}\n",
    "    with open('./models/history_' + str(i) + '.pickle', 'wb') as outp:\n",
    "        pickle.dump(history_data, outp, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    model.load_weights(model_file_name)\n",
    "    \n",
    "    predicted_unit = model.predict(x_test_reshaped) # FINAL FUNCTION CALL\n",
    "    \n",
    "    print(\"Duration (ms):\", duration_ms)\n",
    "\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    max_acc, idx  = max([(val, i) for i, val in enumerate(val_acc)])\n",
    "    val_loss = history.history['val_loss'][idx]\n",
    "    train_acc = history.history['accuracy'][idx]\n",
    "    train_loss = history.history['loss'][idx]\n",
    "\n",
    "    acc = model.evaluate(x_test_reshaped, to_categorical(y_test))[1]\n",
    "   \n",
    "    best_model_file = './acc_finals/CNN_6final_best.wts'\n",
    "    if max_test_acc < acc:\n",
    "        model.save_weights(best_model_file)\n",
    "        max_test_acc = acc\n",
    "        print(max_test_acc)\n",
    "\n",
    "    perGroup_dict['test_acc'] = acc\n",
    "    perGroup_dict['val_acc'] = max_acc\n",
    "    perGroup_dict['val_loss'] = val_loss\n",
    "    perGroup_dict['train_loss'] = train_loss\n",
    "\n",
    "    perGroup_list.append(perGroup_dict)\n",
    "perPartition_file = './acc/perPartition_acc.csv'\n",
    "with open(perPartition_file, \"w\") as csvFile:\n",
    "    writer = csv.DictWriter(csvFile, fieldnames=headers)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(perGroup_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_236\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_236 (Conv2D)          (None, 46, 2, 2)          162       \n",
      "_________________________________________________________________\n",
      "reshape_235 (Reshape)        (None, 92, 2)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_235 (Conv1D)          (None, 45, 2)             18        \n",
      "_________________________________________________________________\n",
      "flatten_235 (Flatten)        (None, 90)                0         \n",
      "_________________________________________________________________\n",
      "dense_235 (Dense)            (None, 27)                2457      \n",
      "=================================================================\n",
      "Total params: 2,637\n",
      "Trainable params: 2,637\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2691 - accuracy: 0.9308\n",
      "0.9307692050933838\n"
     ]
    }
   ],
   "source": [
    "model= create_model_2d(all_classes, filter_width, x_train_reshaped[0].shape)\n",
    "model.summary()\n",
    "model.compile(\n",
    "loss=keras.losses.categorical_crossentropy,\n",
    "                    optimizer=keras.optimizers.Adam(lr=0.0005), ## (0.001) originally\n",
    "                    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "model.load_weights(best_model_file)\n",
    "acc = model.evaluate(x_test_reshaped, to_categorical(y_test))[1]\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### quantized model ###\n",
    "\n",
    "def CreateQModel(all_classes, inp_shape, bit_conv, bit_dense):\n",
    "    x = x_in = Input(inp_shape)\n",
    "    x = QConv2D(2, (4,4), strides = (1,2), \n",
    "    kernel_quantizer = \"quantized_bits(\" + str(bit_conv) + \")\",\n",
    "    bias_quantizer = \"quantized_bits(\" + str(bit_conv) + \")\",\n",
    "    name = \"conv1d_1\") (x)\n",
    "    x = QActivation(\"quantized_relu(\" + str(bit_conv) + \")\", name=\"act_1\")(x)\n",
    "    x = Reshape((-1, 2))(x)\n",
    "\n",
    "    x = QConv1D(2, 4, strides = 2, \n",
    "    kernel_quantizer = \"quantized_bits(\" + str(bit_conv) + \")\",\n",
    "    bias_quantizer = \"quantized_bits(\" + str(bit_conv) + \")\",\n",
    "    name = \"conv1d_2\") (x)\n",
    "    x = QActivation(\"quantized_relu(\" + str(bit_conv) + \")\", name=\"act_2\")(x)\n",
    "\n",
    "    x = Flatten(name=\"flatten\")(x)\n",
    "    x = QDense(all_classes,\n",
    "        kernel_quantizer=\"quantized_bits(\" + str(bit_dense) + \")\",\n",
    "        bias_quantizer=\"quantized_bits(\" + str(bit_dense) + \")\",\n",
    "        name=\"dense\")(x)\n",
    "    x = Activation(\"softmax\", name=\"softmax\")(x)\n",
    "    \n",
    "    model = Model(inputs=x_in, outputs=x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelState(qmodel):\n",
    "    model_ops = extract_model_operations(qmodel)\n",
    "    model_table = dict()\n",
    "    ops_table = defaultdict(lambda: 0)\n",
    "\n",
    "    for name in sorted(model_ops):\n",
    "        mode, _, sizes, signs = model_ops[name][\"type\"]\n",
    "        number = model_ops[name][\"number_of_operations\"]\n",
    "        sign = \"s\" if sum(signs) > 0 else \"u\"\n",
    "        op_name = sign + mode + \"_\" + str(sizes[0]) + \"_\" + str(sizes[1])\n",
    "        ops_table[op_name] += number\n",
    "        model_table[str(name)] = (number, op_name)\n",
    "    \n",
    "    weight_table = dict()\n",
    "\n",
    "    for name in sorted(model_ops):\n",
    "        weight_type = model_ops[name][\"type_of_weights\"]\n",
    "        n_weights = model_ops[name][\"number_of_weights\"]\n",
    "        if isinstance(weight_type, list):\n",
    "            for i, (w_type, w_number) in enumerate(zip(weight_type, n_weights)):\n",
    "                _, w_sizes, _ = w_type\n",
    "                weight_table[str(name) + \"_weights\"] = (w_number, w_sizes)\n",
    "        else:\n",
    "            _, w_sizes, _ = weight_type\n",
    "            weight_table[str(name) + \"_weights\"] = (n_weights, w_sizes)\n",
    "            _, b_sizes, _ = model_ops[name][\"type_of_bias\"]\n",
    "            b_number = model_ops[name][\"number_of_bias\"]\n",
    "            weight_table[str(name) + \"_bias\"] = (b_number, b_sizes)\n",
    "    new_ops = {key:val for key, val in ops_table.items()}\n",
    "    return model_table, new_ops, weight_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perGroup_list = []\n",
    "headers = ['conv_bit', 'dense_bit', 'trial', 'test_acc', 'val_acc', 'val_loss', 'train_loss']\n",
    "conv_bit_list = [8]\n",
    "dense_bit_list = [4]\n",
    "for conv_bit in conv_bit_list:\n",
    "    for dense_bit in dense_bit_list:\n",
    "        if conv_bit >= dense_bit:\n",
    "            for i in range(1):\n",
    "                print(conv_bit, dense_bit)\n",
    "                perGroup_dict = dict()\n",
    "                perGroup_dict['trial'] = i\n",
    "                perGroup_dict['conv_bit'] = conv_bit\n",
    "                perGroup_dict['dense_bit'] = dense_bit\n",
    "                model = CreateQModel(all_classes, x_train_reshaped[0].shape, conv_bit, dense_bit)\n",
    "                ops_table  = modelState(model)\n",
    "\n",
    "                model.compile(\n",
    "                loss=keras.losses.categorical_crossentropy,\n",
    "                                    optimizer=keras.optimizers.Adam(lr=0.0005), ## (0.001) originally\n",
    "                                    metrics=['accuracy'],\n",
    "                )\n",
    "\n",
    "                model_file_name = './qkeras/model/' + str(conv_bit) + '-' + str(dense_bit) + 'qkeras_t' + str(i) + '.wts.h5'\n",
    "\n",
    "                t0 = time.time()\n",
    "                # Train the model.\n",
    "                history = model.fit(\n",
    "                x_train_reshaped,\n",
    "                to_categorical(y_train),\n",
    "                epochs=300,\n",
    "                validation_data=(x_val_reshaped, to_categorical(y_val)),\n",
    "                verbose=1,\n",
    "                callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=150),\n",
    "                    keras.callbacks.ModelCheckpoint(model_file_name, monitor='val_accuracy', verbose=0, save_best_only=True, mode='auto')]\n",
    "                )\n",
    "                t1 = time.time()\n",
    "\n",
    "                history_data = {'epoch':history.epoch, 'history':history.history}\n",
    "\n",
    "                model.load_weights(model_file_name)\n",
    "\n",
    "                predicted_unit = model.predict(x_test_reshaped) # FINAL FUNCTION CALL\n",
    "                \n",
    "                duration_ms = (t1-t0)*1000\n",
    "                print(\"Duration (ms):\", duration_ms)\n",
    "\n",
    "                val_acc = history.history['val_accuracy']\n",
    "                max_acc, idx  = max([(val, i) for i, val in enumerate(val_acc)])\n",
    "                val_loss = history.history['val_loss'][idx]\n",
    "                train_acc = history.history['accuracy'][idx]\n",
    "                train_loss = history.history['loss'][idx]\n",
    "\n",
    "\n",
    "\n",
    "                acc = model.evaluate(x_test_reshaped, to_categorical(y_test))[1]\n",
    "                perGroup_dict['test_acc'] = acc\n",
    "                perGroup_dict['val_acc'] = max_acc\n",
    "                perGroup_dict['val_loss'] = val_loss\n",
    "                perGroup_dict['train_loss'] = train_loss\n",
    "\n",
    "                perGroup_list.append(perGroup_dict)\n",
    "\n",
    "perPartition_file = './qkeras/acc/qkeras_quantized_input_test.csv'\n",
    "\n",
    "with open(perPartition_file, \"w\") as csvFile:\n",
    "    writer = csv.DictWriter(csvFile, fieldnames=headers)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(perGroup_list)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e57d43101a7d2cef50e2886d1731c21991ae54dc38c30b56c1c03be085b3885f"
  },
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
