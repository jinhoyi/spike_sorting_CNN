{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.io as io\n",
    "from sklearn.model_selection import train_test_split\n",
    "from os import path as path\n",
    "import matplotlib.pyplot as plt\n",
    "from math import floor\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Flatten, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import qkeras as qkeras\n",
    "from qkeras import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading Data ###\n",
    "xmat = io.loadmat('xdata_64.mat')\n",
    "ymat = io.loadmat('ydata_64.mat')\n",
    "xs = xmat['x_data']\n",
    "ys = ymat['y_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Include \"multi-unit\"  ###\n",
    "include_multiunit = 1 \n",
    "units = 20 # number of non-multiunit classes\n",
    "samps = xs.shape[0]\n",
    "i = 0\n",
    "while i < xs.shape[0]:\n",
    "    if ys[i] == 0 and not include_multiunit:\n",
    "        ys = np.delete(ys, i, 0)\n",
    "        xs = np.delete(xs, i, 0)\n",
    "        continue\n",
    "    if ys[i][0] > units:\n",
    "        ys = np.delete(ys, i, 0)\n",
    "        xs = np.delete(xs, i, 0)\n",
    "        continue\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not include_multiunit:\n",
    "    for i in range(ys.shape[0]):\n",
    "        ys[i] -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Functions for Data manipulation ###\n",
    "def shift(x):\n",
    "    return x + abs(np.amin(x))\n",
    "\n",
    "def rand_roll_sig(x, min_roll, max_roll):\n",
    "    roll_amt = np.random.randint(min_roll, max_roll+1)\n",
    "    return np.roll(x, roll_amt)\n",
    "\n",
    "def quantize_signal(x, nbits, quant_range):\n",
    "    y = np.empty_like(x)\n",
    "    dv = (quant_range[1]-quant_range[0])/(2**nbits)\n",
    "    for i in range(x.shape[0]):\n",
    "        if x[i] < quant_range[0]:\n",
    "            y[i] = quant_range[0]\n",
    "        elif x[i] > quant_range[1]:\n",
    "            y[i] = quant_range[1]\n",
    "        else:\n",
    "            y[i] = x[i]\n",
    "        y[i] = min(floor(y[i]/dv)*dv, (2**nbits -1) * dv)\n",
    "    return y    \n",
    "\n",
    "def add_noise(orig_sig, desired_SNR_dB):\n",
    "    # Desired SNR in dB\n",
    "    SNR_dB = desired_SNR_dB\n",
    "    # Desired linear SNR\n",
    "    snr = 10.0**(SNR_dB/10.0)\n",
    "    p1 = orig_sig.var()\n",
    "\n",
    "    # Calculate required noise power for desired SNR\n",
    "    n = p1/snr\n",
    "\n",
    "    # Generate noise with calculated power\n",
    "    w = np.sqrt(n)*sp.randn(orig_sig.shape[0])\n",
    "\n",
    "    # Add noise to signal\n",
    "    return orig_sig + w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(2, 2)\n",
    "fig.tight_layout()\n",
    "\n",
    "# ax.yaxis.set_visible(False) # same for y axis.\n",
    "\n",
    "\n",
    "orig = xs[0]\n",
    "res_6 = quantize_signal(shift(xs[0]), 6, [0, 3.5])\n",
    "res_4 = quantize_signal(shift(xs[0]), 4, [0, 3.5])\n",
    "res_2 = quantize_signal(shift(xs[0]), 2, [0, 3.5])\n",
    "\n",
    "ax[0, 0].plot(xs[0], 'b') #row=0, col=0\n",
    "ax[0, 0].set_xlabel(\"(a)\")\n",
    "# ax[0, 0].yaxis.set_visible(False)\n",
    "\n",
    "ax[0, 1].plot(res_6, 'g') #row=0, col=1\n",
    "ax[0, 1].set_xlabel(\"(b)\")\n",
    "# ax[0, 1].yaxis.set_visible(False)\n",
    "\n",
    "ax[1, 0].plot(res_4, 'k') #row=1, col=0\n",
    "ax[1, 0].set_xlabel(\"(c)\")\n",
    "# ax[1, 0].yaxis.set_visible(False)\n",
    "\n",
    "ax[1, 1].plot(res_2, 'r') #row=1, col=1\n",
    "ax[1, 1].set_xlabel(\"(d)\")\n",
    "# ax[1, 1].yaxis.set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_q_n = np.empty_like(xs)\n",
    "for i in range(xs.shape[0]):\n",
    "    sig = xs[i]\n",
    "    \n",
    "    sig = shift(sig)\n",
    "    sig = quantize_signal(sig, nbits=10, quant_range=[0,3.5])\n",
    "    xs_q_n[i] = sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11361, 64, 1) (11361, 1) (1515, 64, 1) (1515, 1) (2273, 64, 1) (2273, 1)\n"
     ]
    }
   ],
   "source": [
    "### Split Data ###\n",
    "all_classes = len(np.unique(ys))\n",
    "\n",
    "\n",
    "train_ratio = 0.75\n",
    "validation_ratio = 0.1\n",
    "test_ratio = 0.15\n",
    "\n",
    "# train is now 75% of the entire data set\n",
    "x_train, x_test, y_train, y_test = train_test_split(xs_q_n, ys, test_size=1 - train_ratio, random_state=2021)\n",
    "\n",
    "# test is now 10% of the initial data set\n",
    "# validation is now 15% of the initial data set\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio), random_state=2021)\n",
    "\n",
    "x_train_reshaped = np.expand_dims(x_train, axis=2)\n",
    "x_test_reshaped = np.expand_dims(x_test, axis=2)\n",
    "x_val_reshaped = np.expand_dims(x_val, axis=2)\n",
    "\n",
    "print(x_train_reshaped.shape, y_train.shape, x_val_reshaped.shape, y_val.shape, x_test_reshaped.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### TensorFlow Model ###\n",
    "def create_model(all_classes, inp_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=4, kernel_size=5, activation='relu', input_shape=inp_shape))\n",
    "    model.add(Conv1D(filters=1, kernel_size=3, activation='relu'))\n",
    "\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(all_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### compile and training ###\n",
    "model = create_model(all_classes, x_train_reshaped[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 2.5723 - accuracy: 0.2740 - val_loss: 1.9004 - val_accuracy: 0.4653\n",
      "Epoch 2/60\n",
      "356/356 [==============================] - 0s 912us/step - loss: 1.1982 - accuracy: 0.6551 - val_loss: 0.7617 - val_accuracy: 0.7934\n",
      "Epoch 3/60\n",
      "356/356 [==============================] - 0s 877us/step - loss: 0.5353 - accuracy: 0.8436 - val_loss: 0.4223 - val_accuracy: 0.8739\n",
      "Epoch 4/60\n",
      "356/356 [==============================] - 0s 893us/step - loss: 0.3184 - accuracy: 0.9087 - val_loss: 0.2809 - val_accuracy: 0.9076\n",
      "Epoch 5/60\n",
      "356/356 [==============================] - 0s 937us/step - loss: 0.2207 - accuracy: 0.9368 - val_loss: 0.2035 - val_accuracy: 0.9373\n",
      "Epoch 6/60\n",
      "356/356 [==============================] - 0s 911us/step - loss: 0.1680 - accuracy: 0.9508 - val_loss: 0.1670 - val_accuracy: 0.9406\n",
      "Epoch 7/60\n",
      "356/356 [==============================] - 0s 908us/step - loss: 0.1372 - accuracy: 0.9585 - val_loss: 0.1343 - val_accuracy: 0.9518\n",
      "Epoch 8/60\n",
      "356/356 [==============================] - 0s 878us/step - loss: 0.1193 - accuracy: 0.9636 - val_loss: 0.1095 - val_accuracy: 0.9604\n",
      "Epoch 9/60\n",
      "356/356 [==============================] - 0s 868us/step - loss: 0.1049 - accuracy: 0.9681 - val_loss: 0.1075 - val_accuracy: 0.9617\n",
      "Epoch 10/60\n",
      "356/356 [==============================] - 0s 882us/step - loss: 0.0925 - accuracy: 0.9710 - val_loss: 0.0963 - val_accuracy: 0.9624\n",
      "Epoch 11/60\n",
      "356/356 [==============================] - 0s 873us/step - loss: 0.0862 - accuracy: 0.9740 - val_loss: 0.0854 - val_accuracy: 0.9630\n",
      "Epoch 12/60\n",
      "356/356 [==============================] - 0s 882us/step - loss: 0.0820 - accuracy: 0.9751 - val_loss: 0.0674 - val_accuracy: 0.9782\n",
      "Epoch 13/60\n",
      "356/356 [==============================] - 0s 854us/step - loss: 0.0732 - accuracy: 0.9787 - val_loss: 0.0682 - val_accuracy: 0.9716\n",
      "Epoch 14/60\n",
      "356/356 [==============================] - 0s 840us/step - loss: 0.0687 - accuracy: 0.9791 - val_loss: 0.0707 - val_accuracy: 0.9729\n",
      "Epoch 15/60\n",
      "356/356 [==============================] - 0s 865us/step - loss: 0.0653 - accuracy: 0.9810 - val_loss: 0.0610 - val_accuracy: 0.9769\n",
      "Epoch 16/60\n",
      "356/356 [==============================] - 0s 876us/step - loss: 0.0628 - accuracy: 0.9807 - val_loss: 0.0500 - val_accuracy: 0.9809\n",
      "Epoch 17/60\n",
      "356/356 [==============================] - 0s 854us/step - loss: 0.0609 - accuracy: 0.9826 - val_loss: 0.0523 - val_accuracy: 0.9795\n",
      "Epoch 18/60\n",
      "356/356 [==============================] - 0s 843us/step - loss: 0.0579 - accuracy: 0.9825 - val_loss: 0.0569 - val_accuracy: 0.9809\n",
      "Epoch 19/60\n",
      "356/356 [==============================] - 0s 829us/step - loss: 0.0570 - accuracy: 0.9828 - val_loss: 0.0620 - val_accuracy: 0.9729\n",
      "Epoch 20/60\n",
      "356/356 [==============================] - 0s 857us/step - loss: 0.0537 - accuracy: 0.9831 - val_loss: 0.0520 - val_accuracy: 0.9809\n",
      "Epoch 21/60\n",
      "356/356 [==============================] - 0s 1000us/step - loss: 0.0527 - accuracy: 0.9838 - val_loss: 0.0637 - val_accuracy: 0.9782\n",
      "Epoch 22/60\n",
      "356/356 [==============================] - 0s 920us/step - loss: 0.0509 - accuracy: 0.9844 - val_loss: 0.0462 - val_accuracy: 0.9828\n",
      "Epoch 23/60\n",
      "356/356 [==============================] - 0s 843us/step - loss: 0.0496 - accuracy: 0.9849 - val_loss: 0.0676 - val_accuracy: 0.9749\n",
      "Epoch 24/60\n",
      "356/356 [==============================] - 0s 950us/step - loss: 0.0508 - accuracy: 0.9849 - val_loss: 0.0455 - val_accuracy: 0.9802\n",
      "Epoch 25/60\n",
      "356/356 [==============================] - 0s 900us/step - loss: 0.0470 - accuracy: 0.9871 - val_loss: 0.0418 - val_accuracy: 0.9828\n",
      "Epoch 26/60\n",
      "356/356 [==============================] - 0s 843us/step - loss: 0.0487 - accuracy: 0.9847 - val_loss: 0.0474 - val_accuracy: 0.9822\n",
      "Epoch 27/60\n",
      "356/356 [==============================] - 0s 840us/step - loss: 0.0455 - accuracy: 0.9861 - val_loss: 0.0450 - val_accuracy: 0.9822\n",
      "Epoch 28/60\n",
      "356/356 [==============================] - 0s 835us/step - loss: 0.0437 - accuracy: 0.9873 - val_loss: 0.0510 - val_accuracy: 0.9802\n",
      "Epoch 29/60\n",
      "356/356 [==============================] - 0s 871us/step - loss: 0.0444 - accuracy: 0.9871 - val_loss: 0.0390 - val_accuracy: 0.9842\n",
      "Epoch 30/60\n",
      "356/356 [==============================] - 0s 992us/step - loss: 0.0437 - accuracy: 0.9872 - val_loss: 0.0439 - val_accuracy: 0.9802\n",
      "Epoch 31/60\n",
      "356/356 [==============================] - 0s 997us/step - loss: 0.0436 - accuracy: 0.9868 - val_loss: 0.0435 - val_accuracy: 0.9848\n",
      "Epoch 32/60\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.0420 - accuracy: 0.9879 - val_loss: 0.0465 - val_accuracy: 0.9815\n",
      "Epoch 33/60\n",
      "356/356 [==============================] - 0s 936us/step - loss: 0.0432 - accuracy: 0.9865 - val_loss: 0.0347 - val_accuracy: 0.9861\n",
      "Epoch 34/60\n",
      "356/356 [==============================] - 0s 848us/step - loss: 0.0406 - accuracy: 0.9887 - val_loss: 0.0353 - val_accuracy: 0.9861\n",
      "Epoch 35/60\n",
      "356/356 [==============================] - 0s 855us/step - loss: 0.0408 - accuracy: 0.9876 - val_loss: 0.0583 - val_accuracy: 0.9815\n",
      "Epoch 36/60\n",
      "356/356 [==============================] - 0s 840us/step - loss: 0.0406 - accuracy: 0.9886 - val_loss: 0.0380 - val_accuracy: 0.9822\n",
      "Epoch 37/60\n",
      "356/356 [==============================] - 0s 926us/step - loss: 0.0394 - accuracy: 0.9880 - val_loss: 0.0337 - val_accuracy: 0.9868\n",
      "Epoch 38/60\n",
      "356/356 [==============================] - 0s 910us/step - loss: 0.0406 - accuracy: 0.9879 - val_loss: 0.0406 - val_accuracy: 0.9835\n",
      "Epoch 39/60\n",
      "356/356 [==============================] - 0s 902us/step - loss: 0.0388 - accuracy: 0.9887 - val_loss: 0.0391 - val_accuracy: 0.9842\n",
      "Epoch 40/60\n",
      "356/356 [==============================] - 0s 868us/step - loss: 0.0398 - accuracy: 0.9874 - val_loss: 0.0493 - val_accuracy: 0.9809\n",
      "Epoch 41/60\n",
      "356/356 [==============================] - 0s 958us/step - loss: 0.0380 - accuracy: 0.9880 - val_loss: 0.0336 - val_accuracy: 0.9875\n",
      "Epoch 42/60\n",
      "356/356 [==============================] - 0s 927us/step - loss: 0.0390 - accuracy: 0.9888 - val_loss: 0.0408 - val_accuracy: 0.9835\n",
      "Epoch 43/60\n",
      "356/356 [==============================] - 0s 848us/step - loss: 0.0381 - accuracy: 0.9889 - val_loss: 0.0420 - val_accuracy: 0.9828\n",
      "Epoch 44/60\n",
      "356/356 [==============================] - 0s 865us/step - loss: 0.0377 - accuracy: 0.9884 - val_loss: 0.0440 - val_accuracy: 0.9842\n",
      "Epoch 45/60\n",
      "356/356 [==============================] - 0s 921us/step - loss: 0.0368 - accuracy: 0.9897 - val_loss: 0.0432 - val_accuracy: 0.9822\n",
      "Epoch 46/60\n",
      "356/356 [==============================] - 0s 868us/step - loss: 0.0344 - accuracy: 0.9903 - val_loss: 0.0365 - val_accuracy: 0.9861\n",
      "Epoch 47/60\n",
      "356/356 [==============================] - 0s 908us/step - loss: 0.0368 - accuracy: 0.9894 - val_loss: 0.0284 - val_accuracy: 0.9875\n",
      "Epoch 48/60\n",
      "356/356 [==============================] - 0s 842us/step - loss: 0.0360 - accuracy: 0.9900 - val_loss: 0.0537 - val_accuracy: 0.9769\n",
      "Epoch 49/60\n",
      "356/356 [==============================] - 0s 857us/step - loss: 0.0356 - accuracy: 0.9893 - val_loss: 0.0279 - val_accuracy: 0.9894\n",
      "Epoch 50/60\n",
      "356/356 [==============================] - 0s 840us/step - loss: 0.0354 - accuracy: 0.9893 - val_loss: 0.0396 - val_accuracy: 0.9815\n",
      "Epoch 51/60\n",
      "356/356 [==============================] - 0s 849us/step - loss: 0.0351 - accuracy: 0.9893 - val_loss: 0.0404 - val_accuracy: 0.9842\n",
      "Epoch 52/60\n",
      "356/356 [==============================] - 0s 837us/step - loss: 0.0355 - accuracy: 0.9892 - val_loss: 0.0433 - val_accuracy: 0.9861\n",
      "Epoch 53/60\n",
      "356/356 [==============================] - 0s 840us/step - loss: 0.0370 - accuracy: 0.9879 - val_loss: 0.0386 - val_accuracy: 0.9848\n",
      "Epoch 54/60\n",
      "356/356 [==============================] - 0s 863us/step - loss: 0.0331 - accuracy: 0.9896 - val_loss: 0.0383 - val_accuracy: 0.9875\n",
      "Epoch 55/60\n",
      "356/356 [==============================] - 0s 879us/step - loss: 0.0350 - accuracy: 0.9894 - val_loss: 0.0381 - val_accuracy: 0.9868\n",
      "Epoch 56/60\n",
      "356/356 [==============================] - 0s 885us/step - loss: 0.0366 - accuracy: 0.9891 - val_loss: 0.0533 - val_accuracy: 0.9802\n",
      "Epoch 57/60\n",
      "356/356 [==============================] - 0s 879us/step - loss: 0.0348 - accuracy: 0.9887 - val_loss: 0.0248 - val_accuracy: 0.9888\n",
      "Epoch 58/60\n",
      "356/356 [==============================] - 0s 961us/step - loss: 0.0312 - accuracy: 0.9913 - val_loss: 0.0310 - val_accuracy: 0.9888\n",
      "Epoch 59/60\n",
      "356/356 [==============================] - 0s 896us/step - loss: 0.0348 - accuracy: 0.9886 - val_loss: 0.0399 - val_accuracy: 0.9835\n",
      "Epoch 60/60\n",
      "356/356 [==============================] - 0s 840us/step - loss: 0.0340 - accuracy: 0.9901 - val_loss: 0.0394 - val_accuracy: 0.9875\n",
      "72/72 [==============================] - 0s 542us/step - loss: 0.0258 - accuracy: 0.9930\n",
      "0.9929608702659607\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "  loss='categorical_crossentropy',\n",
    "  optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "model_file_name = './model/synthetic_full.wts.h5'\n",
    "\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "  x_train_reshaped,\n",
    "  to_categorical(y_train),\n",
    "  epochs=60,\n",
    "  batch_size=32,\n",
    "  validation_data=(x_val_reshaped, to_categorical(y_val)),\n",
    "  callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10),\n",
    "                    keras.callbacks.ModelCheckpoint(model_file_name, monitor='val_loss', verbose=0, save_best_only=True, mode='auto')]\n",
    ")\n",
    "\n",
    "model.load_weights(model_file_name)\n",
    "\n",
    "predicted_unit = model.predict(x_test_reshaped) # FINAL FUNCTION CALL\n",
    "\n",
    "acc = model.evaluate(x_test_reshaped, to_categorical(y_test))[1]\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### qKeras Model ###\n",
    "def CreateQModel(shape, nb_classes):\n",
    "    x = x_in = Input(shape)\n",
    "\n",
    "    x = QConv1D(4, 5,\n",
    "        kernel_quantizer=\"quantized_po2(12)\", \n",
    "        bias_quantizer=\"quantized_po2(8)\",\n",
    "        name=\"conv1d_1\")(x)\n",
    "    x = QActivation(\"quantized_relu(12)\", name=\"act_1\")(x)\n",
    "    \n",
    "    x = QConv1D(1, 3,\n",
    "        kernel_quantizer=\"quantized_po2(12)\", \n",
    "        bias_quantizer=\"quantized_po2(8)\",\n",
    "        name=\"conv1d_2\")(x)\n",
    "    x = QActivation(\"quantized_relu(12)\", name=\"act_2\")(x)    \n",
    "\n",
    "    x = MaxPooling1D(3)(x)\n",
    "    x = Flatten(name=\"flatten\")(x)\n",
    "    x = QDense(nb_classes,\n",
    "        kernel_quantizer=\"quantized_po2(4,4,1)\",\n",
    "        bias_quantizer=\"quantized_po2(4,4,1)\",\n",
    "        name=\"dense\")(x)\n",
    "    x = Activation(\"softmax\", name=\"softmax\")(x)\n",
    "    \n",
    "    model = Model(inputs=x_in, outputs=x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### compile and training ###\n",
    "qmodel2 = CreateQModel(x_train_reshaped[0].shape, len(np.unique(y_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "356/356 [==============================] - 1s 2ms/step - loss: 2.7787 - accuracy: 0.1700 - val_loss: 2.4258 - val_accuracy: 0.3135\n",
      "Epoch 2/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 2.1770 - accuracy: 0.3745 - val_loss: 1.9359 - val_accuracy: 0.4462\n",
      "Epoch 3/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 1.8016 - accuracy: 0.4582 - val_loss: 1.6618 - val_accuracy: 0.5069\n",
      "Epoch 4/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 1.5732 - accuracy: 0.4987 - val_loss: 1.4832 - val_accuracy: 0.5003\n",
      "Epoch 5/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 1.4222 - accuracy: 0.5478 - val_loss: 1.3661 - val_accuracy: 0.5855\n",
      "Epoch 6/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 1.3442 - accuracy: 0.5482 - val_loss: 1.3182 - val_accuracy: 0.5828\n",
      "Epoch 7/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 1.2686 - accuracy: 0.5709 - val_loss: 1.2183 - val_accuracy: 0.5545\n",
      "Epoch 8/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 1.1796 - accuracy: 0.6099 - val_loss: 1.1476 - val_accuracy: 0.6172\n",
      "Epoch 9/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 1.1194 - accuracy: 0.6409 - val_loss: 1.1093 - val_accuracy: 0.6911\n",
      "Epoch 10/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 1.0667 - accuracy: 0.6664 - val_loss: 1.0360 - val_accuracy: 0.7195\n",
      "Epoch 11/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 1.0330 - accuracy: 0.6765 - val_loss: 0.9914 - val_accuracy: 0.7281\n",
      "Epoch 12/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 1.0188 - accuracy: 0.6824 - val_loss: 1.0507 - val_accuracy: 0.6442\n",
      "Epoch 13/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 1.0249 - accuracy: 0.6743 - val_loss: 0.9584 - val_accuracy: 0.7320\n",
      "Epoch 14/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.9887 - accuracy: 0.6897 - val_loss: 1.0437 - val_accuracy: 0.6667\n",
      "Epoch 15/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.9715 - accuracy: 0.6633 - val_loss: 1.0076 - val_accuracy: 0.7155\n",
      "Epoch 16/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.8724 - accuracy: 0.7313 - val_loss: 0.8094 - val_accuracy: 0.7795\n",
      "Epoch 17/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.8047 - accuracy: 0.7787 - val_loss: 0.7803 - val_accuracy: 0.7881\n",
      "Epoch 18/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.7727 - accuracy: 0.7902 - val_loss: 0.7506 - val_accuracy: 0.8178\n",
      "Epoch 19/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.7510 - accuracy: 0.7992 - val_loss: 0.7652 - val_accuracy: 0.7558\n",
      "Epoch 20/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.7212 - accuracy: 0.7987 - val_loss: 0.6917 - val_accuracy: 0.8251\n",
      "Epoch 21/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.6846 - accuracy: 0.8206 - val_loss: 0.6800 - val_accuracy: 0.8218\n",
      "Epoch 22/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.6628 - accuracy: 0.8343 - val_loss: 0.6477 - val_accuracy: 0.8574\n",
      "Epoch 23/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.6634 - accuracy: 0.8329 - val_loss: 0.6381 - val_accuracy: 0.8535\n",
      "Epoch 24/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.6332 - accuracy: 0.8468 - val_loss: 0.7045 - val_accuracy: 0.8224\n",
      "Epoch 25/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.6301 - accuracy: 0.8464 - val_loss: 0.6255 - val_accuracy: 0.8462\n",
      "Epoch 26/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.6266 - accuracy: 0.8497 - val_loss: 0.6340 - val_accuracy: 0.8620\n",
      "Epoch 27/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.6632 - accuracy: 0.8202 - val_loss: 0.6085 - val_accuracy: 0.8548\n",
      "Epoch 28/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.6511 - accuracy: 0.8258 - val_loss: 0.6445 - val_accuracy: 0.8231\n",
      "Epoch 29/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.6430 - accuracy: 0.8296 - val_loss: 0.6268 - val_accuracy: 0.8376\n",
      "Epoch 30/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.6016 - accuracy: 0.8617 - val_loss: 0.6017 - val_accuracy: 0.8508\n",
      "Epoch 31/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.6079 - accuracy: 0.8472 - val_loss: 0.6540 - val_accuracy: 0.8238\n",
      "Epoch 32/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.5900 - accuracy: 0.8484 - val_loss: 0.6080 - val_accuracy: 0.8449\n",
      "Epoch 33/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.5863 - accuracy: 0.8509 - val_loss: 0.6158 - val_accuracy: 0.8172\n",
      "Epoch 34/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.5658 - accuracy: 0.8573 - val_loss: 0.5997 - val_accuracy: 0.8535\n",
      "Epoch 35/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.5781 - accuracy: 0.8563 - val_loss: 0.5805 - val_accuracy: 0.8554\n",
      "Epoch 36/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.6591 - accuracy: 0.8081 - val_loss: 0.6597 - val_accuracy: 0.7934\n",
      "Epoch 37/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.6504 - accuracy: 0.8069 - val_loss: 0.7591 - val_accuracy: 0.7419\n",
      "Epoch 38/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.6285 - accuracy: 0.8177 - val_loss: 0.7502 - val_accuracy: 0.7492\n",
      "Epoch 39/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.6254 - accuracy: 0.8196 - val_loss: 0.7031 - val_accuracy: 0.7789\n",
      "Epoch 40/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.6333 - accuracy: 0.8097 - val_loss: 0.9450 - val_accuracy: 0.6660\n",
      "Epoch 41/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.7160 - accuracy: 0.7733 - val_loss: 0.5505 - val_accuracy: 0.8568\n",
      "Epoch 42/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.5590 - accuracy: 0.8475 - val_loss: 0.5358 - val_accuracy: 0.8627\n",
      "Epoch 43/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.5677 - accuracy: 0.8428 - val_loss: 0.5381 - val_accuracy: 0.8548\n",
      "Epoch 44/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.5348 - accuracy: 0.8645 - val_loss: 0.5119 - val_accuracy: 0.8799\n",
      "Epoch 45/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.5485 - accuracy: 0.8538 - val_loss: 0.4953 - val_accuracy: 0.8680\n",
      "Epoch 46/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.5501 - accuracy: 0.8469 - val_loss: 0.5006 - val_accuracy: 0.8686\n",
      "Epoch 47/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.5599 - accuracy: 0.8365 - val_loss: 0.7174 - val_accuracy: 0.7703\n",
      "Epoch 48/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.5583 - accuracy: 0.8412 - val_loss: 0.5444 - val_accuracy: 0.8350\n",
      "Epoch 49/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.5252 - accuracy: 0.8565 - val_loss: 0.6611 - val_accuracy: 0.7723\n",
      "Epoch 50/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.5000 - accuracy: 0.8670 - val_loss: 0.4847 - val_accuracy: 0.8667\n",
      "Epoch 51/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.4914 - accuracy: 0.8723 - val_loss: 0.5084 - val_accuracy: 0.8713\n",
      "Epoch 52/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.4854 - accuracy: 0.8701 - val_loss: 0.5457 - val_accuracy: 0.8330\n",
      "Epoch 53/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.4807 - accuracy: 0.8731 - val_loss: 0.5381 - val_accuracy: 0.8363\n",
      "Epoch 54/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.4512 - accuracy: 0.8870 - val_loss: 0.4425 - val_accuracy: 0.8957\n",
      "Epoch 55/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.4388 - accuracy: 0.9005 - val_loss: 0.4209 - val_accuracy: 0.9142\n",
      "Epoch 56/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.4153 - accuracy: 0.9104 - val_loss: 0.4342 - val_accuracy: 0.9023\n",
      "Epoch 57/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.4138 - accuracy: 0.9069 - val_loss: 0.4143 - val_accuracy: 0.9063\n",
      "Epoch 58/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.3976 - accuracy: 0.9132 - val_loss: 0.3974 - val_accuracy: 0.9155\n",
      "Epoch 59/200\n",
      "356/356 [==============================] - 1s 1ms/step - loss: 0.3977 - accuracy: 0.9078 - val_loss: 0.4003 - val_accuracy: 0.9003\n",
      "Epoch 60/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.3695 - accuracy: 0.9173 - val_loss: 0.4597 - val_accuracy: 0.8865\n",
      "Epoch 61/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.3708 - accuracy: 0.9167 - val_loss: 0.3686 - val_accuracy: 0.9109\n",
      "Epoch 62/200\n",
      "356/356 [==============================] - 1s 1ms/step - loss: 0.3518 - accuracy: 0.9232 - val_loss: 0.3381 - val_accuracy: 0.9234\n",
      "Epoch 63/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.3479 - accuracy: 0.9206 - val_loss: 0.3189 - val_accuracy: 0.9307\n",
      "Epoch 64/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.3426 - accuracy: 0.9264 - val_loss: 0.3729 - val_accuracy: 0.9050\n",
      "Epoch 65/200\n",
      "356/356 [==============================] - 1s 1ms/step - loss: 0.3413 - accuracy: 0.9263 - val_loss: 0.4274 - val_accuracy: 0.8785\n",
      "Epoch 66/200\n",
      "356/356 [==============================] - 1s 1ms/step - loss: 0.3212 - accuracy: 0.9291 - val_loss: 0.3145 - val_accuracy: 0.9228\n",
      "Epoch 67/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.3163 - accuracy: 0.9312 - val_loss: 0.3470 - val_accuracy: 0.9380\n",
      "Epoch 68/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2995 - accuracy: 0.9355 - val_loss: 0.9316 - val_accuracy: 0.6667\n",
      "Epoch 69/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2885 - accuracy: 0.9396 - val_loss: 0.2927 - val_accuracy: 0.9413\n",
      "Epoch 70/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2722 - accuracy: 0.9437 - val_loss: 0.2820 - val_accuracy: 0.9393\n",
      "Epoch 71/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2786 - accuracy: 0.9399 - val_loss: 0.2425 - val_accuracy: 0.9564\n",
      "Epoch 72/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2852 - accuracy: 0.9318 - val_loss: 0.4769 - val_accuracy: 0.8429\n",
      "Epoch 73/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2990 - accuracy: 0.9259 - val_loss: 0.3686 - val_accuracy: 0.9175\n",
      "Epoch 74/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.3239 - accuracy: 0.9145 - val_loss: 0.2561 - val_accuracy: 0.9360\n",
      "Epoch 75/200\n",
      "356/356 [==============================] - 1s 1ms/step - loss: 0.3189 - accuracy: 0.9193 - val_loss: 0.3918 - val_accuracy: 0.8647\n",
      "Epoch 76/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.3094 - accuracy: 0.9183 - val_loss: 0.3529 - val_accuracy: 0.8950\n",
      "Epoch 77/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.3280 - accuracy: 0.9118 - val_loss: 0.3621 - val_accuracy: 0.8812\n",
      "Epoch 78/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.3332 - accuracy: 0.9087 - val_loss: 0.3020 - val_accuracy: 0.9254\n",
      "Epoch 79/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.3183 - accuracy: 0.9203 - val_loss: 0.3352 - val_accuracy: 0.9215\n",
      "Epoch 80/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.3287 - accuracy: 0.9162 - val_loss: 0.2704 - val_accuracy: 0.9360\n",
      "Epoch 81/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.3288 - accuracy: 0.9109 - val_loss: 0.2580 - val_accuracy: 0.9472\n",
      "Epoch 82/200\n",
      "356/356 [==============================] - 1s 1ms/step - loss: 0.3143 - accuracy: 0.9192 - val_loss: 0.2423 - val_accuracy: 0.9492\n",
      "Epoch 83/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.3380 - accuracy: 0.9078 - val_loss: 0.3118 - val_accuracy: 0.9274\n",
      "Epoch 84/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.3576 - accuracy: 0.8992 - val_loss: 0.3439 - val_accuracy: 0.9109\n",
      "Epoch 85/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.3570 - accuracy: 0.8966 - val_loss: 0.2597 - val_accuracy: 0.9333\n",
      "Epoch 86/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.3160 - accuracy: 0.9117 - val_loss: 0.2843 - val_accuracy: 0.9221\n",
      "Epoch 87/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.3069 - accuracy: 0.9154 - val_loss: 0.4843 - val_accuracy: 0.8257\n",
      "Epoch 88/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.3108 - accuracy: 0.9103 - val_loss: 0.3039 - val_accuracy: 0.9142\n",
      "Epoch 89/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.3200 - accuracy: 0.9082 - val_loss: 0.3307 - val_accuracy: 0.8970\n",
      "Epoch 90/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.3304 - accuracy: 0.9041 - val_loss: 0.3038 - val_accuracy: 0.9043\n",
      "Epoch 91/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.9089 - val_loss: 0.3029 - val_accuracy: 0.9234\n",
      "Epoch 92/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.3272 - accuracy: 0.9075 - val_loss: 0.2775 - val_accuracy: 0.9274\n",
      "Epoch 93/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.3272 - accuracy: 0.9026 - val_loss: 0.3716 - val_accuracy: 0.8878\n",
      "Epoch 94/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.3291 - accuracy: 0.9011 - val_loss: 0.5086 - val_accuracy: 0.8165\n",
      "Epoch 95/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.3299 - accuracy: 0.8990 - val_loss: 0.4129 - val_accuracy: 0.8488\n",
      "Epoch 96/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2930 - accuracy: 0.9207 - val_loss: 0.2937 - val_accuracy: 0.9228\n",
      "Epoch 97/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.3034 - accuracy: 0.9144 - val_loss: 0.3941 - val_accuracy: 0.8845\n",
      "Epoch 98/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2970 - accuracy: 0.9170 - val_loss: 0.2352 - val_accuracy: 0.9485\n",
      "Epoch 99/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2851 - accuracy: 0.9203 - val_loss: 0.4034 - val_accuracy: 0.8667\n",
      "Epoch 100/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2951 - accuracy: 0.9148 - val_loss: 0.6326 - val_accuracy: 0.8046\n",
      "Epoch 101/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2936 - accuracy: 0.9141 - val_loss: 0.2215 - val_accuracy: 0.9485\n",
      "Epoch 102/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2815 - accuracy: 0.9204 - val_loss: 0.3341 - val_accuracy: 0.9155\n",
      "Epoch 103/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2843 - accuracy: 0.9213 - val_loss: 0.3212 - val_accuracy: 0.9149\n",
      "Epoch 104/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2875 - accuracy: 0.9163 - val_loss: 0.3006 - val_accuracy: 0.9208\n",
      "Epoch 105/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2817 - accuracy: 0.9221 - val_loss: 0.3750 - val_accuracy: 0.8812\n",
      "Epoch 106/200\n",
      "356/356 [==============================] - 1s 2ms/step - loss: 0.2711 - accuracy: 0.9245 - val_loss: 0.2497 - val_accuracy: 0.9267\n",
      "Epoch 107/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2754 - accuracy: 0.9226 - val_loss: 0.2843 - val_accuracy: 0.9188\n",
      "Epoch 108/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2721 - accuracy: 0.9257 - val_loss: 0.2385 - val_accuracy: 0.9347\n",
      "Epoch 109/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2721 - accuracy: 0.9276 - val_loss: 0.3234 - val_accuracy: 0.9030\n",
      "Epoch 110/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2729 - accuracy: 0.9238 - val_loss: 0.2690 - val_accuracy: 0.9320\n",
      "Epoch 111/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2728 - accuracy: 0.9222 - val_loss: 0.3546 - val_accuracy: 0.8990\n",
      "Epoch 112/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2752 - accuracy: 0.9217 - val_loss: 0.4121 - val_accuracy: 0.8700\n",
      "Epoch 113/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2898 - accuracy: 0.9155 - val_loss: 0.2278 - val_accuracy: 0.9479\n",
      "Epoch 114/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2755 - accuracy: 0.9206 - val_loss: 0.2980 - val_accuracy: 0.8983\n",
      "Epoch 115/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2775 - accuracy: 0.9193 - val_loss: 0.3125 - val_accuracy: 0.8891\n",
      "Epoch 116/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2635 - accuracy: 0.9262 - val_loss: 0.3651 - val_accuracy: 0.8792\n",
      "Epoch 117/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2450 - accuracy: 0.9348 - val_loss: 0.2345 - val_accuracy: 0.9406\n",
      "Epoch 118/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2471 - accuracy: 0.9314 - val_loss: 0.2189 - val_accuracy: 0.9518\n",
      "Epoch 119/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2485 - accuracy: 0.9321 - val_loss: 0.3069 - val_accuracy: 0.9096\n",
      "Epoch 120/200\n",
      "356/356 [==============================] - 1s 1ms/step - loss: 0.2422 - accuracy: 0.9353 - val_loss: 0.2162 - val_accuracy: 0.9505\n",
      "Epoch 121/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2424 - accuracy: 0.9351 - val_loss: 0.2037 - val_accuracy: 0.9505\n",
      "Epoch 122/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2302 - accuracy: 0.9423 - val_loss: 0.1854 - val_accuracy: 0.9624\n",
      "Epoch 123/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2332 - accuracy: 0.9402 - val_loss: 0.2447 - val_accuracy: 0.9327\n",
      "Epoch 124/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2280 - accuracy: 0.9394 - val_loss: 0.2576 - val_accuracy: 0.9340\n",
      "Epoch 125/200\n",
      "356/356 [==============================] - 1s 2ms/step - loss: 0.2454 - accuracy: 0.9328 - val_loss: 0.2450 - val_accuracy: 0.9333\n",
      "Epoch 126/200\n",
      "356/356 [==============================] - 1s 2ms/step - loss: 0.2379 - accuracy: 0.9376 - val_loss: 0.1827 - val_accuracy: 0.9663\n",
      "Epoch 127/200\n",
      "356/356 [==============================] - 1s 2ms/step - loss: 0.2408 - accuracy: 0.9330 - val_loss: 0.2178 - val_accuracy: 0.9413\n",
      "Epoch 128/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2435 - accuracy: 0.9325 - val_loss: 0.2251 - val_accuracy: 0.9393\n",
      "Epoch 129/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2471 - accuracy: 0.9330 - val_loss: 0.2556 - val_accuracy: 0.9340\n",
      "Epoch 130/200\n",
      "356/356 [==============================] - 1s 1ms/step - loss: 0.2326 - accuracy: 0.9395 - val_loss: 0.7388 - val_accuracy: 0.7934\n",
      "Epoch 131/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2393 - accuracy: 0.9348 - val_loss: 0.6132 - val_accuracy: 0.8343\n",
      "Epoch 132/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2471 - accuracy: 0.9315 - val_loss: 0.2118 - val_accuracy: 0.9347\n",
      "Epoch 133/200\n",
      "356/356 [==============================] - 1s 2ms/step - loss: 0.2401 - accuracy: 0.9354 - val_loss: 0.2632 - val_accuracy: 0.9261\n",
      "Epoch 134/200\n",
      "356/356 [==============================] - 1s 1ms/step - loss: 0.2409 - accuracy: 0.9335 - val_loss: 0.1963 - val_accuracy: 0.9505\n",
      "Epoch 135/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2337 - accuracy: 0.9386 - val_loss: 0.2170 - val_accuracy: 0.9426\n",
      "Epoch 136/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2343 - accuracy: 0.9379 - val_loss: 0.2163 - val_accuracy: 0.9459\n",
      "Epoch 137/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2285 - accuracy: 0.9396 - val_loss: 0.2620 - val_accuracy: 0.9287\n",
      "Epoch 138/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2451 - accuracy: 0.9332 - val_loss: 0.2355 - val_accuracy: 0.9380\n",
      "Epoch 139/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2513 - accuracy: 0.9309 - val_loss: 0.2428 - val_accuracy: 0.9294\n",
      "Epoch 140/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2274 - accuracy: 0.9400 - val_loss: 0.2538 - val_accuracy: 0.9241\n",
      "Epoch 141/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2377 - accuracy: 0.9361 - val_loss: 0.2089 - val_accuracy: 0.9545\n",
      "Epoch 142/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2307 - accuracy: 0.9371 - val_loss: 0.2172 - val_accuracy: 0.9518\n",
      "Epoch 143/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2359 - accuracy: 0.9372 - val_loss: 0.2363 - val_accuracy: 0.9406\n",
      "Epoch 144/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2278 - accuracy: 0.9398 - val_loss: 0.3172 - val_accuracy: 0.9129\n",
      "Epoch 145/200\n",
      "356/356 [==============================] - 1s 1ms/step - loss: 0.2406 - accuracy: 0.9362 - val_loss: 0.2275 - val_accuracy: 0.9465\n",
      "Epoch 146/200\n",
      "356/356 [==============================] - 1s 2ms/step - loss: 0.2172 - accuracy: 0.9451 - val_loss: 0.2441 - val_accuracy: 0.9459\n",
      "Epoch 147/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2360 - accuracy: 0.9389 - val_loss: 0.2015 - val_accuracy: 0.9531\n",
      "Epoch 148/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2264 - accuracy: 0.9429 - val_loss: 0.2439 - val_accuracy: 0.9287\n",
      "Epoch 149/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2304 - accuracy: 0.9377 - val_loss: 0.2399 - val_accuracy: 0.9333\n",
      "Epoch 150/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2294 - accuracy: 0.9377 - val_loss: 0.2516 - val_accuracy: 0.9380\n",
      "Epoch 151/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2262 - accuracy: 0.9381 - val_loss: 0.2277 - val_accuracy: 0.9380\n",
      "Epoch 152/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2379 - accuracy: 0.9335 - val_loss: 0.2436 - val_accuracy: 0.9353\n",
      "Epoch 153/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2488 - accuracy: 0.9302 - val_loss: 0.2629 - val_accuracy: 0.9281\n",
      "Epoch 154/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2297 - accuracy: 0.9378 - val_loss: 0.2368 - val_accuracy: 0.9274\n",
      "Epoch 155/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2209 - accuracy: 0.9404 - val_loss: 0.2046 - val_accuracy: 0.9584\n",
      "Epoch 156/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2265 - accuracy: 0.9398 - val_loss: 0.2639 - val_accuracy: 0.9162\n",
      "Epoch 157/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2308 - accuracy: 0.9338 - val_loss: 0.2343 - val_accuracy: 0.9320\n",
      "Epoch 158/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2364 - accuracy: 0.9329 - val_loss: 0.2424 - val_accuracy: 0.9287\n",
      "Epoch 159/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2490 - accuracy: 0.9284 - val_loss: 0.2537 - val_accuracy: 0.9215\n",
      "Epoch 160/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2519 - accuracy: 0.9279 - val_loss: 0.5613 - val_accuracy: 0.8026\n",
      "Epoch 161/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2406 - accuracy: 0.9341 - val_loss: 0.2095 - val_accuracy: 0.9446\n",
      "Epoch 162/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2478 - accuracy: 0.9292 - val_loss: 0.3305 - val_accuracy: 0.8990\n",
      "Epoch 163/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2533 - accuracy: 0.9281 - val_loss: 0.2026 - val_accuracy: 0.9465\n",
      "Epoch 164/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2576 - accuracy: 0.9259 - val_loss: 0.2066 - val_accuracy: 0.9485\n",
      "Epoch 165/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2515 - accuracy: 0.9259 - val_loss: 0.2036 - val_accuracy: 0.9479\n",
      "Epoch 166/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2493 - accuracy: 0.9286 - val_loss: 0.3157 - val_accuracy: 0.8964\n",
      "Epoch 167/200\n",
      "356/356 [==============================] - 1s 2ms/step - loss: 0.2667 - accuracy: 0.9189 - val_loss: 0.2462 - val_accuracy: 0.9287\n",
      "Epoch 168/200\n",
      "356/356 [==============================] - 1s 1ms/step - loss: 0.2634 - accuracy: 0.9199 - val_loss: 0.1880 - val_accuracy: 0.9564\n",
      "Epoch 169/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2547 - accuracy: 0.9243 - val_loss: 0.2359 - val_accuracy: 0.9281\n",
      "Epoch 170/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2657 - accuracy: 0.9200 - val_loss: 0.2370 - val_accuracy: 0.9294\n",
      "Epoch 171/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2675 - accuracy: 0.9166 - val_loss: 0.3742 - val_accuracy: 0.8759\n",
      "Epoch 172/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2432 - accuracy: 0.9290 - val_loss: 0.3559 - val_accuracy: 0.8752\n",
      "Epoch 173/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2707 - accuracy: 0.9144 - val_loss: 0.3700 - val_accuracy: 0.8812\n",
      "Epoch 174/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2569 - accuracy: 0.9236 - val_loss: 0.2393 - val_accuracy: 0.9320\n",
      "Epoch 175/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2552 - accuracy: 0.9209 - val_loss: 0.2332 - val_accuracy: 0.9234\n",
      "Epoch 176/200\n",
      "356/356 [==============================] - 0s 1ms/step - loss: 0.2368 - accuracy: 0.9327 - val_loss: 0.2270 - val_accuracy: 0.9327\n",
      "72/72 [==============================] - 0s 713us/step - loss: 0.1829 - accuracy: 0.9600\n",
      "0.9599648118019104\n"
     ]
    }
   ],
   "source": [
    "qmodel2.compile(\n",
    "  loss='categorical_crossentropy',\n",
    "  optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "model_file_name_q = './model/synthetic_q.wts.h5'\n",
    "\n",
    "\n",
    "# Train the model.\n",
    "qmodel2.fit(\n",
    "  x_train_reshaped,\n",
    "  to_categorical(y_train),\n",
    "  epochs=200,\n",
    "  batch_size=32,\n",
    "  validation_data=(x_val_reshaped, to_categorical(y_val)),\n",
    "  callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50),\n",
    "                    keras.callbacks.ModelCheckpoint(model_file_name_q, monitor='val_loss', verbose=0, save_best_only=True, mode='auto')]\n",
    ")\n",
    "\n",
    "qmodel2.load_weights(model_file_name_q)\n",
    "\n",
    "predicted_unit = qmodel2.predict(x_test_reshaped) # FINAL FUNCTION CALL\n",
    "\n",
    "acc = qmodel2.evaluate(x_test_reshaped, to_categorical(y_test))[1]\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e57d43101a7d2cef50e2886d1731c21991ae54dc38c30b56c1c03be085b3885f"
  },
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
